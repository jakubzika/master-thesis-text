\pagelayout{margin}
\setchapterstyle{kao}
\setchapterpreamble[u]{\margintoc}
\chapter{Research and implementation}
\label{ch:research}

\begin{figure}[hb]
    \includegraphics[width=1\textwidth]{diagram-detail}
\end{figure}

\footnotetext{Title image provides a high level simplified overview of our problem}


In this chapter, we formulate the research problem of estimating electric vehicle charging demand and present our approach to solving it. We begin with a problem statement, followed by a detailed description of our feature engineering process. We then introduce our neural network architecture with latent profiles, which is designed to capture both temporal and spatial patterns in charging behavior. Finally, we explain our dataset splitting strategy, training procedure, and the baseline models used for comparative evaluation.


\section{Problem statement}

Our primary objective is to estimate the \acrfull{APC} of electric vehicle charging stations based on a set of spatial and temporal features. Through this research, we aim to determine whether our data-driven approach is viable for predicting charging demand at new locations.

Formally, we define our model $f_\theta$ as a function that maps from a feature space to a target space:

\[
    f_{\theta}: X \rightarrow Y
\]

Where $X \subset \mathbb{R}^M$ represents the set of all feature vectors (with $M$ dimensions), and $Y \subset \mathbb{R}^{24}$ represents the set of all target vectors (average hourly power consumption over a day). The available data are split into \textit{training}, \textit{test}, and \textit{validation} sets, with the training set used to minimize the empirical risk. The details of this splitting strategy are explained in Section \ref{sec:dataset-split}.



The model function $f$ has trainable parameters $\theta$, which we optimize to minimize the following empirical risk:

\[
    \mathit{loss_{total}}(f,\theta) = \frac{1}{P} \sum_{(x,y)\in\mathcal{T}} \alpha \cdot a_x + \beta \cdot b_x
\]

Where $\alpha$ and $\beta$ are weighting coefficients for the two components of our loss function:

\[
    \begin{split}
        a_x & = \mathit{loss_{power}}
        (\left \lVert f(x_i;\theta) \right \rVert_{1}, \left \lVert y_i \right \rVert_{1}) \\
        b_x & = \mathit{loss_{norm}}
        (\frac{f(x_i;\theta)}{\left \lVert f(x_i;\theta) \right \rVert_{1}}, \frac{y_i}{\left \lVert y_i \right \rVert_{1}})
    \end{split}
\]

The first component, $a_x$, measures the error in predicting the \acrfull{TDPC} using the $L1$ norm of the predicted and actual consumption vectors. The second component, $b_x$, measures the error in predicting the \acrfull{NDPC} by comparing the normalized predicted and actual consumption patterns. This dual-objective loss function aligns with our neural network architecture, which separately models the total power consumption and the normalized consumption pattern.

Mean square error (MSE) has been chose as loss functions for $\text{loss}_\text{power}$ and $\text{loss}_\text{norm}$. This loss function penalizes large difference in predictions more than smaller ones.

\[
    MSE(y,y') = (y - y')^2
\]



\section{Model features and feature engineering}

Our feature vector consists of both \textbf{temporal} and \textbf{spatial} components. Ideally, features regarding the charger's capabilities (particularly maximum power output) would also be included, as these would significantly influence average consumption. However, due to limitations in our current dataset, these features are not available. This limitation motivates our interest in estimating the \acrfull{NDPC}, which may be less dependent on the charger's maximum power output.

The features in our model fall into two categories based on data type: categorical and numerical. We process these feature types as follows:

\textbf{Categorical features} are transformed using one-hot encoding. This technique converts a categorical feature with $n$ possible values into $n$ binary features, where each binary feature corresponds to one possible value. For each observation, exactly one of these binary features has the value 1, indicating the presence of that categorical value, while all others are 0.

\marginnote{For example, if we have a categorical feature "day of week" with 7 possible values (Monday through Sunday), one-hot encoding transforms this into 7 binary features: "is\_Monday", "is\_Tuesday", etc. If an observation occurs on Wednesday, then the "is\_Wednesday" feature would be 1, while all other day features would be 0. This transformation allows the model to properly handle categorical variables without imposing an arbitrary ordinal relationship between category values.}

In our model, categorical features such as day of the week, month, and location characteristics are one-hot encoded before being fed into the neural network. This ensures that the model can effectively learn from these categorical variables without imposing arbitrary numerical relationships.

\textbf{Numerical features} are standardized by subtracting the mean and dividing by the standard deviation. This normalization ensures all features are on a comparable scale, preventing features with larger magnitudes from dominating the learning process and helping achieve faster convergence during training.

The feature vector $x_i$ for each sample is constructed as follows:

\[
    \renewcommand*{\arraystretch}{1.5}
    x_i = \begin{bmatrix}
        s_i^T \\
        t_i^T
    \end{bmatrix}
\]

Where $s_i$ represents the spatial features and $t_i$ represents the temporal features.

The \textbf{spatial features} vector $s_i$ is defined as:

\[
    s_i = \begin{bmatrix}
        s_i^1  \\
        \vdots \\
        s_i^R
    \end{bmatrix}
\]

Table \ref{tab:spatial-features-table} provides a detailed overview of the spatial features used in our model.

\begin{table*}[h!]
    \def\arraystretch{1.5}
    \caption{Overview of spatial features used in the feature vector. The additional processing is described in Section \ref{sec:spatial-transformations}}
    \label{tab:spatial-features-table}
    \begin{tabular}{p{1cm} p{3.5cm} p{1.8cm} p{3.5cm} p{4.5cm}}
        \toprule
        \textbf{Index} & \textbf{Name}                                                       & \textbf{Type} & \textbf{Value from}                  & \textbf{Additional processing}                                                                                             \\
        \midrule
        $s_{1}$        & ZSJ population                                                      & numeric       & charger in ZSJ polygon               & normalization by the polygon area                                                                                          \\
        $s_{2:10}$     & ZSJ type                                                            & categorical   & charger in ZSJ polygon               & one-hot encoding                                                                                                           \\
        $s_{11}$       & ZSJ number of addresses                                             & numeric       & charger in ZSJ polygon               & normalization by the polygon area                                                                                          \\
        $s_{12}$       & Number of people commuting into the district from inside Prague     & numeric       & charger in the district polygon      & normalization by the polygon area                                                                                          \\
        $s_{13}$       & Number of people commuting into the district from outside of Prague & numeric       & charger in the district polygon      & normalization by the polygon area                                                                                          \\
        $s_{14:162}$   & Points of Interest                                                  & numeric       & number of PoIs by euclidean distance & importance calculation (value of single PoI is 1 if its distance from charger is 0, 0 if it is of distance 2km or further) \\
        \bottomrule
    \end{tabular}
\end{table*}

The \textbf{temporal features} vector $t_i$ is defined as:

\[
    t_i = \begin{bmatrix}
        t_i^1  \\
        \vdots \\
        t_i^P
    \end{bmatrix}
\]

Table \ref{tab:temporal-features-table} provides a detailed overview of the temporal features used in our model.

\begin{table*}[h!]
    \caption{Overview of temporal features used in the feature vector.}
    \label{tab:temporal-features-table}
    \begin{tabular}{p{1cm} p{3.5cm} p{1.8cm} p{3.5cm} p{4.5cm}}
        \toprule
        \textbf{Index} & \textbf{Name}   & \textbf{Type} & \textbf{Value from} & \textbf{Additional processing} \\
        \midrule
        $t_{1:7}$      & day of the week & categorical   & \acrfull{APC}       & one-hot encoding               \\
        $t_{8:19}$     & month           & categorical   & \acrfull{APC}       & one-hot encoding               \\
        \bottomrule
    \end{tabular}
\end{table*}

\section{Architecture of the Latent Neural Network}

Our machine learning problem formulation allows for various potential solutions, particularly within the class of neural networks. Before introducing our specific architecture, let's briefly review the fundamentals of neural networks.

Neural networks are computational models consisting of layers of interconnected nodes or "neurons" that process information. A typical neural network contains an input layer that receives data, one or more hidden layers that perform computations, and an output layer that produces the final result. Each connection between neurons has an associated weight that is adjusted during the training process. Information flows through the network via activation functions, which introduce non-linearity and allow the network to learn complex patterns. The training process involves feeding the network with labeled examples and using optimization algorithms, typically variants of gradient descent, to minimize a loss function by adjusting the weights. Backpropagation is the primary algorithm used to calculate gradients and update weights efficiently.

For our specific problem, we propose a neural network with latent profiles, as illustrated in Figure \ref{fig:nn-latent}. The key innovation in our architecture is the construction of a latent profile matrix $R \in \mathbb{R}^{24 \times K}$, where $K$ is the number of latent profiles. The network learns these profiles and predicts how they should be combined to generate the final consumption pattern for a given location and time period.

\newpage

The network utilizes the following layers:

\newcommand{\nnmodule}[3]{%
    \begin{marginfigure}[1cm]
        \centering
        \includegraphics[width=1.8cm]{#2}
    \end{marginfigure}
    \item \textbf{#1} \\
    #3
    \vspace{2mm}
}


\begin{itemize}
    \item[] \textbf{Trainable layers:}
          \begin{itemize}
              \nnmodule{Fully connected (Linear transformation)}{nn-modules/fully-conn.pdf}{%
                  $\text{Linear}^n_m(x) = W x + b$ \\
                  $\text{Linear}^n_m: \mathbb{R}^n \rightarrow \mathbb{R}^m \; , \;
                      W \in \mathbb{R}^{m \times n} \; , \;
                      b \in \mathbb{R}^m$ \\
                  $W, b$ are learnable parameters \\

                  \vspace{2mm}

                  This is a standard linear transformation that maps input vectors to output vectors through a weight matrix and bias vector.
              }

              \nnmodule{Latent vectors (Embedding)}{nn-modules/grad-parameter.pdf}{%
                  $\text{LatentVec}_K = R$ \\
                  $\text{LatentVec}_K: \emptyset \rightarrow \mathbb{R}^{24 \times K}$ \\
                  $R$ is learnable

                  \vspace{2mm}

                  This layer represents our latent profiles matrix, which contains $K$ different 24-hour consumption patterns that the network learns during training.
              }

          \end{itemize}

    \item[] \textbf{Non-parametric operations:}
          \begin{itemize}
              \nnmodule{Softplus (Smooth activation)}{nn-modules/softplus.pdf}{%
                  $f(x) = \ln(1 + e^x)$ \\
                  $f: \mathbb{R} \rightarrow \mathbb{R}^+$

                  \vspace{2mm}

                  Softplus is a smooth approximation of the ReLU activation function. It ensures that the output is always positive, which is appropriate for our case since power consumption cannot be negative.
              }

              \nnmodule{Normalization}{nn-modules/norm.pdf}{%
                  $\text{Norm}(x) = \frac{x}{\|x\|_2}$ \\
                  $\text{Norm}: \mathbb{R}^n \rightarrow \{y \in \mathbb{R}^n : \|y\|_2 = 1\}$ \\
                  input dimension matches output dimension \\

                  \vspace{2mm}

                  This operation normalizes vectors to have unit L2 norm. We use it to normalize our latent profiles and to compute the normalized daily power consumption pattern.
              }

              \nnmodule{Leaky-relu}{nn-modules/leaky-relu.pdf}{%
                  $\text{LReLU}(x) = \begin{cases}
                          x        & \text{if } x > 0    \\
                          \alpha x & \text{if } x \leq 0
                      \end{cases}$ \\
                  $\text{LReLU}: \mathbb{R} \rightarrow \mathbb{R} \; , \; \alpha \text{ is a hyperparameter}$ \\

                  \vspace{2mm}

                  Leaky ReLU is an extension of the standard ReLU activation function that allows a small gradient when the unit is not active. In this work, there is no clear motivation for its use over tanh or standard ReLU.
              }
          \end{itemize}
\end{itemize}

\vspace{5mm}

\newpage

These layers are combined to form three functional modules, each with a specific purpose in our neural network architecture:

\begin{itemize}
    \item[] \textbf{Network modules:}
          \begin{itemize}
              \nnmodule{f module (Latent profile probabilities)}{nn-modules/f-module.pdf}{%
                  $f: \mathbb{R}^d \rightarrow \mathbb{R}^K$ \\
                  $f = \text{Softmax} \circ \text{Linear}_{K} \circ \text{LeakyReLU} \circ \text{Linear}_{64} \circ \text{LeakyReLU} \circ \text{Linear}_{h}$ \\
                  Where $d$ is feature size, $h$ is hidden size, and $K$ is latent profiles count \\
                  Outputs normalized weights for latent profiles

                  \vspace{3mm}

                  The purpose of this module is to predict the contribution of individual latent profiles to the resulting normalized consumption pattern. In other words, this module estimates the daily rhythm of the charger without considering the actual total power. The input is a feature vector, which is transformed by two linear layers with LeakyReLU activations. The output is a vector of $K$ values transformed by softmax to ensure the sum equals 1, representing the mixing weights for the latent profiles.
              }

              \nnmodule{g module (Total power)}{nn-modules/g-module.pdf}{%
                  $g: \mathbb{R}^d \rightarrow \mathbb{R}$ \\
                  $g = \text{Linear}_{1} \circ \text{LeakyReLU} \circ \text{Linear}_{32} \circ \text{LeakyReLU} \circ \text{Linear}_{h_g}$ \\
                  Where $d$ is feature size and $h_g$ is hidden size for g module

                  \vspace{3mm}

                  This module predicts the total power consumption. It consists of three linear layers joined with LeakyReLU activations. Its input is a feature vector, and it outputs a single scalar. This scalar is multiplied with the combined output of the h module to obtain the final prediction, effectively scaling the normalized consumption pattern to the appropriate magnitude.
              }

              \nnmodule{h module (Latent profiles)}{nn-modules/h-module.pdf}{%
                  $h(\mathbf{x}) = f(\mathbf{x}) \cdot R^T$ \\
                  $h: \mathbb{R}^d \rightarrow \mathbb{R}^{24}$ \\
                  Where $R \in \mathbb{R}^{24 \times K}$ is the normalized latent profiles matrix \\
                  $24$ is the time granularity (hours in a day), $K$ is latent profiles count

                  \vspace{3mm}

                  The h module provides
              }
          \end{itemize}
\end{itemize}

\vspace{5mm}

\newpage

The outputs of the h and f modules are combined as follows:

\[
    \begin{split}
         & \text{Combine}(R,p) = \sum_{i=1}^K p_i \, R_i                                            \\
         & \text{Combine}: \mathbb{R}^{24 \times K} \times \mathbb{R}^K \rightarrow \mathbb{R}^{24}
    \end{split}
\]

This combined output is then multiplied by the scalar from the g module to produce the final prediction of hourly power consumption.

\begin{figure}[hb]
    \includegraphics[width=0.8\textwidth]{nn-latent-architecture.pdf}
    \caption[Latent Neural Network Architecture]{Latent neural network architecture. Light blue rectangles denote NN layers without trainable parameters, while blue denotes layers with parameters learned by SGD. Notation borrowed from Fleuret's book "Little Book of Deep Learning"}
    \label{fig:nn-latent}
\end{figure}

The model is implemented in Python using the PyTorch library
\sidecite{paszke2019pytorchimperativestylehighperformance}.

\newpage


\section{Dataset splitting}
\label{sec:dataset-split}

Our dataset splitting strategy is designed to evaluate the model's ability to generalize to new locations, which is crucial for our use case of predicting demand for new charging stations.

We split our data into three sets: training, test, and validation. However, we cannot simply randomly assign samples to these sets because of the hierarchical nature of our data. Specifically, one physical location may have multiple charging points (CPs) within a charging station (CS), and each CP may have multiple average power consumption (APC) measurements for various temporal patterns (TP). There might be high correlation between measurements from the same location, which would lead to data leakage if we split randomly.

To address this issue, we split the data based on location. Features for each unique location can only be present in one of the sets. This ensures that when we evaluate our model on the test or validation set, it is truly being tested on locations it has never seen during training.

% \[
%     \begin{split}
%         \textit{train}:      \; \mathcal{T} & = ((x_i, y_i) \in X \times Y |\, i = 1,\dots, P) \\
%         \textit{test}:       \; \mathcal{S} & = ((x_i, y_i) \in X \times Y |\, i = 1,\dots, R) \\
%         \textit{validation}: \; \mathcal{V} & = ((x_i, y_i) \in X \times Y |\, i = 1,\dots, O)
%     \end{split}
% \]

The three datasets serve distinct purposes in our research:

\begin{itemize}
    \item \textbf{Train dataset} (72\% of the whole dataset) is used for training the model using \acrlong{SGD} for empirical risk minimization.
          \[
              \label{eq:train}
              \textit{train}: \; \mathcal{T} = \{(x_i, y_i) \in X \times Y |\, i = 1,\dots, P\}
          \]
          \vspace{3mm}


    \item \textbf{Test dataset} (21\% of the whole dataset) is used for inspecting the model's performance on unseen data and tuning hyperparameters. It provides an estimate of the true model risk.

          \[
              \label{eq:test}
              \textit{test}: \; \mathcal{S} = \{(x_i, y_i) \in X \times Y |\, i = 1,\dots, R\}
          \]
          \vspace{3mm}
    \item \textbf{Validation dataset} (7\% of the whole dataset) is used to obtain the final model risk assessment on a model with already trained parameters and chosen hyperparameters.
          \[
              \label{eq:validation}
              \textit{validation}: \; \mathcal{V} = \{(x_i, y_i) \in X \times Y |\, i = 1,\dots, O\}
          \]
\end{itemize}

\section{Training procedure}

To train our neural network model, we utilize the standard PyTorch training procedure with mini-batch \acrfull{SGD} optimization. The model is trained iteratively over multiple epochs, with early stopping implemented to prevent overfitting.


\section{Other models for quantitative comparison}

To evaluate the effectiveness of our latent neural network approach, we compare it quantitatively with other machine learning models. The results from this comparison can either motivate further research with our model or indicate that simpler approaches may be sufficient.

One challenge in this comparison arises from our custom loss function, which combines two L1 losses. The baseline models are trained to minimize standard L1 loss, which might seem to create an unfair comparison. However, we provide more detailed analysis of this issue in Chapter \ref{ch:results}.

The models we use for comparison are:

\begin{itemize}
    \item \textbf{Average} - The simplest baseline, which takes the average over all feature vectors in the dataset and uses that value for all predictions. Both the train and test datasets $\mathcal{T}$ and $\mathcal{S}$ are utilized for this "model." It does not take into account the input features and uses a single value for all predictions.

          The implementation is straightforward:
          \[
              \begin{split}
                   & f_{\mathcal{D}}(z) = \frac{1}{|\mathcal{D}|} \sum_{(x,\_) \in \mathcal{D}} x \\
                   & f: \mathbb{R}^m \rightarrow \mathbb{R}^n                                     \\
                   & n,m \in \mathbb{N}, \mathcal{D} \in (X \times Y)^p, p \in \mathbb{N}
              \end{split}
          \]


    \item \textbf{Linear regression} - A standard linear model that learns a linear mapping from input features to output predictions.

          We use the implementation from the Python library Scikit-Learn \sidecite{scikit-learn}:
          \[
              \begin{split}
                   & f(z) = \alpha + \beta z                                    \\
                   & f: \mathbb{R}^m \rightarrow \mathbb{R}^n                   \\
                   & \alpha \in \mathbb{R}^n, \beta \in \mathbb{R}^{n \times m}
              \end{split}
          \]
    \item \textbf{XGBoost} - A scalable end-to-end tree boosting system \sidecite{Chen_2016}.
\end{itemize}

The performance of these models is compared with our latent neural network based on mean absolute error and mean square error for three metrics: \acrlong{APC} (hourly consumption), \acrlong{TDPC} (total daily consumption), and \acrlong{NDPC} (normalized daily consumption pattern).
